# ğŸ¬ Local Movie Recommender using LLaMA 3.2 (via Ollama)

This is a minimal Python project that gives you **movie recommendations** based on your preferred **language** and **genre**, using a locally running **LLaMA 3.2 model via Ollama** â€” all on your own machine.  
No API keys. No cloud. Fully private.

---

## ğŸ§  What It Does

- Asks for your input (e.g., â€œJapanese Thrillerâ€)
- Sends a custom prompt to the local LLaMA 3.2 model
- Receives a list of **10 movie suggestions** with short summaries
- Displays them as a clean **Markdown table** (best in Jupyter or terminal Markdown viewers)

---

## âœ… Why Local LLMs?

Running models **locally** has its perks:

- ğŸ” **No API key needed** â€” fully self-contained
- ğŸ“´ **Works offline** â€” no internet = no problem
- ğŸ‘ï¸ **Privacy-first** â€” your inputs stay on your machine
- ğŸ’¸ **Free & Open** â€” no rate limits, no cost per request

This small project shows how powerful local models can be, even on limited hardware.

---

## âš ï¸ Limitations of Local CPU-based LLMs

While it's cool to run LLMs locally, especially using Ollama + CPU:

- ğŸ¢ Performance may be slow (especially first runs)
- ğŸ’¾ 8GB RAM machines canâ€™t handle huge models like `llama3:70b`  
- ğŸ§  Generation size needs to be controlled (shorter prompts are better)

For lightweight, structured output like this movie recommendation â€” it's perfect! But donâ€™t expect GPT-4-level reasoning on a CPU ğŸ˜…

---

