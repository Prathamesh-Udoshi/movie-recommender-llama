# ğŸ¬ Local LLM-Based Movie Recommender (LLaMA 3 + Ollama)

This project is a simple Python-based movie recommender that uses a **local LLaMA 3.2 model via Ollama** to suggest **top-rated movies** based on your preferred **language** and **genre**. Instead of using any online API or cloud-based service, it prompts a locally running large language model (LLM) to generate a table of movie titles and short summaries.

ğŸ’¡ Example:  
You input `"Language: Hindi"` and `"Genre: Thriller"` â€” the model responds with a Markdown table listing relevant movie recommendations like *Cure*, *Audition*, etc., with short plot summaries.

---

## âœ… Advantages of Using a Local LLM

- **No API keys or tokens required** â€“ completely self-contained
- **Data stays on your machine** â€“ nothing is sent to the cloud
- **Offline capability** â€“ works without internet access
- **Customizable prompts** â€“ full control over interaction style
- **No rate limits or costs** â€“ run it as much as you want

---

## âš ï¸ Limitations

- ğŸŒ **Slower inference** compared to cloud models (especially on low-spec machines)
- ğŸ’» **High RAM/CPU usage** â€“ requires decent hardware to run smoothly
- ğŸ“¦ **Large model size** â€“ downloads of 3â€“8 GB are common
- ğŸ“… **No real-time knowledge** â€“ model is only trained on static data

---

## ğŸ¤¯ Why Itâ€™s Cool

Itâ€™s amazing how a few gigabytes of model weights can generate intelligent and personalized recommendations â€” without any internet connection. Your laptop becomes the AI.

This project is a small example of how powerful and private local LLMs can be for building intelligent tools.
